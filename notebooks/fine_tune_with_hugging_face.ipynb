{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FErInREmdTp",
        "outputId": "1b380641-67de-46c8-cc1f-75ac6e38104d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PisUoMdHCDNq"
      },
      "source": [
        "~~~\n",
        "Copyright 2025 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "~~~\n",
        "\n",
        "# Fine-tune MedGemma with Hugging Face\n",
        "\n",
        "<table><tbody><tr>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/google-health/medgemma/blob/main/notebooks/fine_tune_with_hugging_face.ipynb\">\n",
        "      <img alt=\"Google Colab logo\" src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" width=\"32px\"><br> Run in Google Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2Fgoogle-health%2Fmedgemma%2Fmain%2Fnotebooks%2Ffine_tune_with_hugging_face.ipynb\">\n",
        "      <img alt=\"Google Cloud Colab Enterprise logo\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" width=\"32px\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/google-health/medgemma/blob/main/notebooks/fine_tune_with_hugging_face.ipynb\">\n",
        "      <img alt=\"GitHub logo\" src=\"https://github.githubassets.com/assets/GitHub-Mark-ea2971cee799.png\" width=\"32px\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://huggingface.co/collections/google/medgemma-release-680aade845f90bec6a3f60c4\">\n",
        "      <img alt=\"Hugging Face logo\" src=\"https://huggingface.co/front/assets/huggingface_logo-noborder.svg\" width=\"32px\"><br> View on Hugging Face\n",
        "    </a>\n",
        "  </td>\n",
        "</tr></tbody></table>\n",
        "\n",
        "This notebook demonstrates fine-tuning MedGemma on an image and text dataset for a vision task using Hugging Face libraries.\n",
        "\n",
        "In this guide, you will use Hugging Face's [Transformer Reinforcement Learning (`TRL`)](https://github.com/huggingface/trl) library to train the model with Supervised Fine-Tuning (SFT), utilizing [Quantized Low-Rank Adaptation (QLoRA)](https://arxiv.org/abs/2305.14314) to reduce computational costs while maintaining high performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcamD7TMKjbt"
      },
      "source": [
        "## Setup\n",
        "\n",
        "To complete this tutorial, you'll need to have a runtime with sufficient resources to fine-tune the MedGemma model. **Note:** This guide requires a GPU that supports bfloat16 data type and has at least 40 GB of memory.\n",
        "\n",
        "You can run this notebook in Google Colab using an A100 GPU:\n",
        "\n",
        "1. In the upper-right of the Colab window, select **‚ñæ (Additional connection options)**.\n",
        "2. Select **Change runtime type**.\n",
        "3. Under **Hardware accelerator**, select **A100 GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF2vUYdJKlCZ"
      },
      "source": [
        "### Get access to MedGemma\n",
        "\n",
        "Before you get started, make sure that you have access to MedGemma models on Hugging Face:\n",
        "\n",
        "1. If you don't already have a Hugging Face account, you can create one for free by clicking [here](https://huggingface.co/join).\n",
        "2. Head over to the [MedGemma model page](https://huggingface.co/google/medgemma-4b-it) and accept the usage conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syRJ6pl4KpEL"
      },
      "source": [
        "### Configure your HF token\n",
        "\n",
        "Generate a Hugging Face `write` access token by going to [settings](https://huggingface.co/settings/tokens). **Note:** Make sure that the token has write access to push the fine-tuned model to Hugging Face Hub.\n",
        "\n",
        "If you are using Google Colab, add your access token to the Colab Secrets manager to securely store it. If not, proceed to run the cell below to authenticate with Hugging Face.\n",
        "\n",
        "1. Open your Google Colab notebook and click on the üîë Secrets tab in the left panel. <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
        "2. Create a new secret with the name `HF_TOKEN`.\n",
        "3. Copy/paste your token key into the Value input box of `HF_TOKEN`.\n",
        "4. Toggle the button on the left to allow notebook access to the secret."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "w8NaHvwRqL3-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules and not os.environ.get(\"VERTEX_PRODUCT\"):\n",
        "    # Use secret if running in Google Colab\n",
        "    from google.colab import userdata\n",
        "    os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "else:\n",
        "    # Store Hugging Face data under `/content` if running in Colab Enterprise\n",
        "    if os.environ.get(\"VERTEX_PRODUCT\") == \"COLAB_ENTERPRISE\":\n",
        "        os.environ[\"HF_HOME\"] = \"/content/hf\"\n",
        "    # Authenticate with Hugging Face\n",
        "    from huggingface_hub import get_token\n",
        "    if get_token() is None:\n",
        "        from huggingface_hub import notebook_login\n",
        "        notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFU3B09TKuQf"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Sgv3DCPIA5p-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee457c14-03a4-40ec-8bd2-0b75a461c3d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m532.9/532.9 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires tensorboard~=2.19.0, but you have tensorboard 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install --upgrade --quiet bitsandbytes datasets evaluate peft tensorboard transformers trl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrnDCZvvzWtS"
      },
      "source": [
        "## Prepare fine-tuning dataset\n",
        "\n",
        "This notebook uses the [NCT-CRC-HE-100K](https://zenodo.org/records/1214456) dataset which is licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode), containing image patches from histological images of human colorectal cancer (CRC) and normal tissue, to fine-tune MedGemma to classify tissue types.\n",
        "\n",
        "**Note:** The full NCT-CRC-HE-100K dataset contains 100K samples. By default this guide only uses a subset with 10,000 samples to keep the training example small, but you can adjust this number if you want to experiment.\n",
        "\n",
        "**Dataset citation:** Kather, J. N., Halama, N., & Marx, A. (2018). 100,000 histological images of human colorectal cancer and healthy tissue (v0.1) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.1214456"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q6p2msnQ1rE"
      },
      "source": [
        "Download the dataset. This step may take around 15 minutes to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SKIP Downloading if it exists on your drive**"
      ],
      "metadata": {
        "id": "arS-LSor30ks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lr3Iy7rq85LF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c66c810-e743-4425-d23d-4836801f5905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 9318M  100 9318M    0     0  18.2M      0  0:08:32  0:08:32 --:--:-- 19.2M\n"
          ]
        }
      ],
      "source": [
        "# ! wget -nc -q \"https://zenodo.org/records/1214456/files/NCT-CRC-HE-100K.zip\"\n",
        "# ! unzip -q NCT-CRC-HE-100K.zip\n",
        "! curl -O \"https://isic-archive.s3.amazonaws.com/challenges/2019/ISIC_2019_Training_Input.zip\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! curl -O \"https://isic-archive.s3.amazonaws.com/challenges/2019/ISIC_2019_Training_Metadata.csv\"\n",
        "! curl -O \"https://isic-archive.s3.amazonaws.com/challenges/2019/ISIC_2019_Training_GroundTruth.csv\""
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6nEep8BzsTV",
        "outputId": "79cf134f-7f5d-4df1-ef6d-a8cf3e91ba7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1185k  100 1185k    0     0   717k      0  0:00:01  0:00:01 --:--:--  717k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1261k  100 1261k    0     0   749k      0  0:00:01  0:00:01 --:--:--  749k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! rm -rf ISIC_2019_Training_Input\n",
        "! unzip -q ISIC_2019_Training_Input.zip"
      ],
      "metadata": {
        "id": "M7Utbdf8WQCN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! rm ISIC_2019_Training_Input.zip"
      ],
      "metadata": {
        "id": "4WARkmwGmCNl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define source paths\n",
        "image_dir_source = \"ISIC_2019_Training_Input\"\n",
        "metadata_csv_source = \"ISIC_2019_Training_Metadata.csv\"\n",
        "groundtruth_csv_source = \"ISIC_2019_Training_GroundTruth.csv\"\n",
        "\n",
        "# Define destination directory in Google Drive\n",
        "drive_path = \"/content/drive/My Drive/ISIC_2019_Dataset\"\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "print(f\"Moving '{image_dir_source}' to '{drive_path}'...\")\n",
        "shutil.move(image_dir_source, drive_path)\n",
        "\n",
        "print(f\"Moving '{metadata_csv_source}' to '{drive_path}'...\")\n",
        "shutil.move(metadata_csv_source, drive_path)\n",
        "\n",
        "print(f\"Moving '{groundtruth_csv_source}' to '{drive_path}'...\")\n",
        "shutil.move(groundtruth_csv_source, drive_path)\n",
        "\n",
        "print(\"All specified files and folders have been moved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "hl6WOmSf1O61",
        "outputId": "93573eac-f87b-45cb-cb3c-71d09e1b5725"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving 'ISIC_2019_Training_Input' to '/content/drive/My Drive/ISIC_2019_Dataset'...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "Destination path '/content/drive/My Drive/ISIC_2019_Dataset/ISIC_2019_Training_Input' already exists",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3743512709.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Moving '{image_dir_source}' to '{drive_path}'...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrive_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Moving '{metadata_csv_source}' to '{drive_path}'...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    846\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Destination path '/content/drive/My Drive/ISIC_2019_Dataset/ISIC_2019_Training_Input' already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y4tiZQF71vwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c037328"
      },
      "source": [
        "train_size = 22265  # @param {type: \"number\"}\n",
        "validation_size = 3066  # @param {type: \"number\"}\n",
        "\n",
        "# Data loading and splitting is now handled in a separate cell below\n",
        "# Removed: data = load_dataset(\"./ISIC_2019_Training_Input\", split=\"train\")\n",
        "# Removed: data = data.train_test_split(train_size=train_size, test_size=validation_size, shuffle=True, seed=42,)\n",
        "# Removed: data[\"validation\"] = data.pop(\"test\")\n",
        "# Removed: data"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "ba41e5f9",
        "outputId": "f8be62af-e21d-4a40-d67b-fe59e6c5a4d3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the ground truth CSV to inspect its structure\n",
        "ground_truth_df = pd.read_csv(\"/content/drive/My Drive/ISIC_2019_Dataset/ISIC_2019_Training_GroundTruth.csv\")\n",
        "display(ground_truth_df.head())\n",
        "\n",
        "# Get the label names from the columns, excluding the 'image' column\n",
        "ISIC_TISSUE_CLASSES_RAW = ground_truth_df.columns.drop(\"image\").tolist()\n",
        "print(f\"Detected ISIC Tissue Classes: {ISIC_TISSUE_CLASSES_RAW}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          image  MEL   NV  BCC   AK  BKL   DF  VASC  SCC  UNK\n",
              "0  ISIC_0000000  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "1  ISIC_0000001  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "2  ISIC_0000002  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "3  ISIC_0000003  0.0  1.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0\n",
              "4  ISIC_0000004  1.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e6ea2e2b-a8c4-4336-83a1-1759ec26b0ef\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>MEL</th>\n",
              "      <th>NV</th>\n",
              "      <th>BCC</th>\n",
              "      <th>AK</th>\n",
              "      <th>BKL</th>\n",
              "      <th>DF</th>\n",
              "      <th>VASC</th>\n",
              "      <th>SCC</th>\n",
              "      <th>UNK</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ISIC_0000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ISIC_0000001</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ISIC_0000002</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ISIC_0000003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ISIC_0000004</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6ea2e2b-a8c4-4336-83a1-1759ec26b0ef')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e6ea2e2b-a8c4-4336-83a1-1759ec26b0ef button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e6ea2e2b-a8c4-4336-83a1-1759ec26b0ef');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(f\\\"Detected ISIC Tissue Classes: {ISIC_TISSUE_CLASSES_RAW}\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"ISIC_0000001\",\n          \"ISIC_0000004\",\n          \"ISIC_0000002\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MEL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5477225575051662,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NV\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5477225575051662,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BCC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AK\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BKL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"VASC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SCC\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"UNK\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected ISIC Tissue Classes: ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC', 'UNK']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58d0e76a"
      },
      "source": [
        "Now that we have inspected the ground truth data, we can update the `TISSUE_CLASSES` and then load the image dataset and merge it with the labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1567c43c",
        "outputId": "62c29795-25ec-46d0-9d4c-28eb6b59b8fb"
      },
      "source": [
        "from datasets import Image, load_dataset, Dataset\n",
        "import os\n",
        "\n",
        "# Define the new TISSUE_CLASSES based on the ground truth CSV\n",
        "TISSUE_CLASSES = [f\"{chr(65+i)}: {cls_name}\" for i, cls_name in enumerate(ISIC_TISSUE_CLASSES_RAW)]\n",
        "\n",
        "options = \"\\n\".join(TISSUE_CLASSES)\n",
        "\n",
        "\n",
        "# Create a mapping from image ID to its integer label\n",
        "def get_label_from_ground_truth(image_id: str) -> int:\n",
        "    row = ground_truth_df[ground_truth_df['image'] == image_id]\n",
        "    if not row.empty:\n",
        "        # Find the column with value 1.0 (indicating the class)\n",
        "        label_col = row.drop(columns='image').iloc[0]\n",
        "        label_index = label_col[label_col == 1.0].index.tolist()\n",
        "        if label_index:\n",
        "            return ISIC_TISSUE_CLASSES_RAW.index(label_index[0])\n",
        "    return -1 # Should not happen if all images have a label\n",
        "\n",
        "# Load image paths and create a list of dictionaries with 'image_path' and 'label'\n",
        "image_dir = \"/content/drive/My Drive/ISIC_2019_Dataset/ISIC_2019_Training_Input\"\n",
        "image_data_list = []\n",
        "for filename in os.listdir(image_dir):\n",
        "    if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "        image_id = os.path.splitext(filename)[0]\n",
        "        label = get_label_from_ground_truth(image_id)\n",
        "        if label != -1:\n",
        "            image_data_list.append({\"image_path\": os.path.join(image_dir, filename), \"label\": label})\n",
        "\n",
        "# Create a Dataset from the list and cast the image column\n",
        "dataset_with_paths_and_labels = Dataset.from_list(image_data_list).cast_column(\"image_path\", Image())\n",
        "\n",
        "# Rename 'image_path' to 'image' to match original notebook structure\n",
        "dataset_with_paths_and_labels = dataset_with_paths_and_labels.rename_column(\"image_path\", \"image\")\n",
        "\n",
        "# Create train and validation splits\n",
        "data = dataset_with_paths_and_labels.train_test_split(\n",
        "    train_size=train_size,\n",
        "    test_size=validation_size,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        ")\n",
        "# Use the test split as the validation set\n",
        "data[\"validation\"] = data.pop(\"test\")\n",
        "\n",
        "# Display dataset details\n",
        "data"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 22265\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['image', 'label'],\n",
              "        num_rows: 3066\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(TISSUE_CLASSES)\n",
        "print(data['train'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV2Xl7Pp6fgN",
        "outputId": "bf2c1601-bc02-46d3-a76d-41119d88d40f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A: MEL', 'B: NV', 'C: BCC', 'D: AK', 'E: BKL', 'F: DF', 'G: VASC', 'H: SCC', 'I: UNK']\n",
            "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x450 at 0x79F7DB0F3500>, 'label': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "793cb085",
        "outputId": "848c084d-252c-43e3-f3ca-115e30bc9b8b"
      },
      "source": [
        "from typing import Any\n",
        "\n",
        "# TISSUE_CLASSES is now defined in the previous cell based on ISIC data\n",
        "# options and PROMPT are also defined in the previous cell\n",
        "\n",
        "PROMPT = f\"What is the most likely condition in the given dermascopic image?\\n{options}\"\n",
        "print(\"using prompt:\", PROMPT,\"options\",TISSUE_CLASSES)\n",
        "\n",
        "\n",
        "def format_data(example: dict[str, Any]) -> dict[str, Any]:\n",
        "    example[\"messages\"] = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": PROMPT,\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": TISSUE_CLASSES[example[\"label\"]],\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    return example"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using prompt: What is the most likely condition in the given dermascopic image?\n",
            "A: MEL\n",
            "B: NV\n",
            "C: BCC\n",
            "D: AK\n",
            "E: BKL\n",
            "F: DF\n",
            "G: VASC\n",
            "H: SCC\n",
            "I: UNK options ['A: MEL', 'B: NV', 'C: BCC', 'D: AK', 'E: BKL', 'F: DF', 'G: VASC', 'H: SCC', 'I: UNK']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0dlyBQdpAAD"
      },
      "source": [
        "For this classification task, create a multiple-choice question prompt and preprocess the data into a multimodal conversational format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBf5lbmkJ59x"
      },
      "source": [
        "Apply the processing function on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "fFiPNp30_X6z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206,
          "referenced_widgets": [
            "1b743679e2dd41fc9c0f40f96e3f5638",
            "6360f3f99cd744b68d807a6453138c8d",
            "8e317d173460425282626390218a3ec2",
            "a93179fe020f4ba79a1a0a82f9393d4c",
            "924fbbfaf5604ce6881ccbdad739150d",
            "3edc25e8609f48eca1f93022128ff1ad",
            "4b39329e821e418ba4e32c7e07b44bc5",
            "9f4efb1878e84ebc9eab3801093804d8",
            "6017c8de1ef24c16a624f8ab55b1fe8b",
            "41f9af04ccc347faa649beba0e1b5729",
            "1c1f28cafc324a59b6a49daceda0bec8",
            "7bd590bf677049c988e13d9b54278540",
            "aca81fcf598f4cc2b3c9f5791fd1d962",
            "a6ebf9aae3fc499981d34be75f32b6b9",
            "0990c563b146486b9ec061f27c0c1493",
            "d93213d572a047c6ad02b1d092dae6e9",
            "f746f4a967f34048a818e8ee3306f7e4",
            "efabe38218e14596a7569900a8a576a7",
            "cce693c63cef4345b8ffe54bdacd2f79",
            "20cbfd051ab7465a875a03bb076ac093",
            "81f988c2d5ba48cc82e9655ecb0f1ac9",
            "8204f1b072154ebc9657289955faad76"
          ]
        },
        "outputId": "724dcec5-ba71-4a70-e3bb-0cfd62d1287a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/22265 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b743679e2dd41fc9c0f40f96e3f5638"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3066 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bd590bf677049c988e13d9b54278540"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=600x450>,\n",
              " 'label': 1,\n",
              " 'messages': [{'content': [{'text': None, 'type': 'image'},\n",
              "    {'text': 'What is the most likely condition in the given dermascopic image?\\nA: MEL\\nB: NV\\nC: BCC\\nD: AK\\nE: BKL\\nF: DF\\nG: VASC\\nH: SCC\\nI: UNK',\n",
              "     'type': 'text'}],\n",
              "   'role': 'user'},\n",
              "  {'content': [{'text': 'B: NV', 'type': 'text'}], 'role': 'assistant'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "data = data.map(format_data)\n",
        "\n",
        "# Display a processed data sample\n",
        "data[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHOtCBFfSWyS"
      },
      "source": [
        "## Fine-tune the model with LoRA\n",
        "\n",
        "Traditional fine-tuning of large language models is resource-intensive because it requires adjusting billions of parameters. Parameter-Efficient Fine-Tuning (PEFT) addresses this by training a smaller number of parameters. A common PEFT technique is Low-Rank Adaptation (LoRA), which efficiently adapts large language models by training small, low-rank matrices that are added to the original model instead of updating the full-weight matrices. In QLoRA, the base model is quantized to 4-bit before its weights are frozen, then LoRA adapter layers are attached and trained.\n",
        "\n",
        "This notebook demonstrates supervised fine-tuning MedGemma with QLoRA using the `SFTTrainer` from the Hugging Face `TRL` library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXh0RtBk_Xc6"
      },
      "source": [
        "### Load model from Hugging Face Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lGkKFxVM5Fb"
      },
      "source": [
        "Initialize the quantization configuration and load the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "r-esHCwnQFye",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "93ab2e932ebd4ee0bb989ad75b0090bf",
            "44f46aa80cd447a0a444c2058401584d",
            "8941fc5500644e9b9ba4c8629de5a837",
            "2bf3935cc42d42a8815ae58977c87ab9",
            "a9efd910d2ea4a63a7fe87cb33f6ba50",
            "5a3e06d9fd5d4d8d8db57acf6038c7e4",
            "6c02be820eeb4bd4b5604da2c3f60170",
            "12ba7eefeb014c9d9e0d0729772c8747",
            "0b851ca942874a5c82b72ef2f78302ec",
            "d683d0c67e594dfb96ed473900ef1f54",
            "5e3d4293df7a4a53b81a93684a79eb1a"
          ]
        },
        "outputId": "c5418ee3-20fa-474c-dc00-d1203bbe674e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93ab2e932ebd4ee0bb989ad75b0090bf"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"unsloth/medgemma-1.5-4b-it\"\n",
        "\n",
        "# T4 GPUs support float16 but not bfloat16. Setting to float16.\n",
        "# If you have an A100/H100 GPU (compute capability >= 8.0), you can use torch.bfloat16.\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.float16, # Changed from torch.bfloat16\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
        "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# Use right padding to avoid issues during training\n",
        "processor.tokenizer.padding_side = \"right\"\n",
        "torch.cuda.empty_cache() # Added to clear memory after model loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQmPZGy_7NV"
      },
      "source": [
        "### Set up for fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jlPOkUjhH1P"
      },
      "source": [
        "Create a [`LoraConfig`](https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig). It will be provided to the `SFTTrainer`, which supports built-in integration with the Hugging Face `PEFT` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "eCWNm3wtMBqX"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=32,                      # Increased for complex clinical visual features\n",
        "    lora_alpha=32,             # Set to r or 2*r for stability/learning speed\n",
        "    lora_dropout=0.1,          # Slightly higher to prevent overfitting on clinical datasets\n",
        "    bias=\"none\",               # Standard for keeping base behavior frozen\n",
        "    target_modules=\"all-linear\", # Best practice for multimodal LLMs\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # modules_to_save removed to preserve original model behavior\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrDwrz6iD3Yl"
      },
      "source": [
        "Define a custom data collator that processes examples containing text and images and returns batches of data in the expected model input format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "QRPETOTGV81b"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "\n",
        "def collate_fn(examples: list[dict[str, Any]]):\n",
        "    texts = []\n",
        "    images = []\n",
        "    for example in examples:\n",
        "        images.append([example[\"image\"].convert(\"RGB\")])\n",
        "        texts.append(processor.apply_chat_template(\n",
        "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
        "        ).strip())\n",
        "\n",
        "    # Tokenize the texts and process the images\n",
        "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # The labels are the input_ids, with the padding and image tokens masked in\n",
        "    # the loss computation\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "    # Mask image tokens\n",
        "    image_token_id = [\n",
        "        processor.tokenizer.convert_tokens_to_ids(\n",
        "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
        "        )\n",
        "    ]\n",
        "    # Mask tokens that are not used in the loss computation\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "    labels[labels == image_token_id] = -100\n",
        "    labels[labels == 262144] = -100\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsaon7JoBq5H"
      },
      "source": [
        "Configure training parameters in an [`SFTConfig`](https://huggingface.co/docs/trl/sft_trainer#trl.SFTConfig)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "E6W_gQmfRXWx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f321c42-2a56-40c5-a89a-1af8a62f8218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTConfig\n",
        "\n",
        "num_train_epochs = 1  # @param {type: \"number\"}\n",
        "learning_rate = 2e-4  # @param {type: \"number\"}\n",
        "\n",
        "args = SFTConfig(\n",
        "    output_dir=\"medgemma-4b-it-sft-lora-crc100k\",            # Directory and Hub repository id to save the model to\n",
        "    num_train_epochs=num_train_epochs,                       # Number of training epochs\n",
        "    per_device_train_batch_size=1,                           # Batch size per device during training (further reduced)\n",
        "    per_device_eval_batch_size=1,                            # Batch size per device during evaluation (further reduced)\n",
        "    gradient_accumulation_steps=16,                          # Number of steps before performing a backward/update pass (increased)\n",
        "    gradient_checkpointing=True,                             # Enable gradient checkpointing to reduce memory usage\n",
        "    optim=\"adamw_torch_fused\",                               # Use fused AdamW optimizer for better performance\n",
        "    logging_steps=50,                                        # Number of steps between logs\n",
        "    save_strategy=\"epoch\",                                   # Save checkpoint every epoch\n",
        "    eval_strategy=\"steps\",                                   # Evaluate every `eval_steps`\n",
        "    eval_steps=50,                                           # Number of steps between evaluations\n",
        "    learning_rate=learning_rate,                             # Learning rate based on QLoRA paper\n",
        "    fp16=True,                                               # Use float16 precision for T4 GPUs\n",
        "    bf16=False,                                              # Explicitly disable bfloat16\n",
        "    max_grad_norm=0.3,                                       # Max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                                       # Warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"linear\",                              # Use linear learning rate scheduler\n",
        "    push_to_hub=True,                                        # Push model to Hub\n",
        "    report_to=\"tensorboard\",                                 # Report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Set gradient checkpointing to non-reentrant to avoid issues\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},           # Skip default dataset preparation to preprocess manually\n",
        "    remove_unused_columns = False,                           # Columns are unused for training but needed for data collator\n",
        "    label_names=[\"labels\"],                                  # Input keys that correspond to the labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnPGToATAFCN"
      },
      "source": [
        "### Fine-tune the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1rMdSj1Tj1K"
      },
      "source": [
        "Construct an [`SFTTrainer`](https://huggingface.co/docs/trl/sft_trainer) using the previously defined LoRA configuration, custom data collator, and training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dWcynpm0MHDc"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=data[\"train\"],\n",
        "    eval_dataset=data[\"validation\"].shuffle().select(range(200)),  # Use subset of validation set for faster run\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor,\n",
        "    data_collator=collate_fn,\n",
        ")\n",
        "torch.cuda.empty_cache() # Added to clear memory after trainer initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d254661",
        "outputId": "9059b55f-e181-441d-dbf9-e3d253a8b0af"
      },
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "print(\"PYTORCH_CUDA_ALLOC_CONF set to expandable_segments:True\")\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PYTORCH_CUDA_ALLOC_CONF set to expandable_segments:True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoHlpSKKVbDb"
      },
      "source": [
        "Launch the fine-tuning process.\n",
        "\n",
        "**Note**: This may take around 3 hours to run using the default configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "viHUr_jJfHCG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "d2a3a8a3-090a-44ee-98ad-89cf93a627ab"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "\"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4032920361.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2163\u001b[0m                 \u001b[0;31m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2164\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2165\u001b[0;31m                 return inner_training_loop(\n\u001b[0m\u001b[1;32m   2166\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2167\u001b[0m                     \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2567\u001b[0m                                     \u001b[0mgrad_norm_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimplicit_replication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m                                 \u001b[0;32mwith\u001b[0m \u001b[0mgrad_norm_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2569\u001b[0;31m                                     _grad_norm = self.accelerator.clip_grad_norm_(\n\u001b[0m\u001b[1;32m   2570\u001b[0m                                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m                                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   3006\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3009\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36munscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2944\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAcceleratedOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2946\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_dtype_grads\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mper_device_and_dtype_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mper_dtype_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                     torch._amp_foreach_non_finite_check_and_unscale_(\n\u001b[0m\u001b[1;32m    281\u001b[0m                         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                         \u001b[0mper_device_found_inf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: \"_amp_foreach_non_finite_check_and_unscale_cuda\" not implemented for 'BFloat16'"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBotQy2xAU6-"
      },
      "source": [
        "Save the final model to Hugging Face Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-WxJHtQZJII"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcRXx3qwAdjd"
      },
      "source": [
        "Free up memory before proceeding to evaluate and test the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El9e2_8xZLLi"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf9QkPFAfegp"
      },
      "source": [
        "## Evaluate the fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMZCfth8Nn3O"
      },
      "source": [
        "### Prepare test dataset\n",
        "\n",
        "The [CRC-VAL-HE-7K](https://zenodo.org/records/1214456) dataset contains image patches from patients with colorectal adenocarcinoma and does not overlap with NCT-CRC-HE-100K. It can be used as the test dataset to evaluate the fine-tuned MedGemma model.\n",
        "\n",
        "**Note:** The full CRC-VAL-HE-7K dataset contains over 7K samples. By default this guide only uses a subset with 1,000 samples to keep the evaluation example small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eziDK33PdB5f"
      },
      "source": [
        "Download and prepare the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lv82XT9fVI6y"
      },
      "outputs": [],
      "source": [
        "! wget -nc -q \"https://zenodo.org/records/1214456/files/CRC-VAL-HE-7K.zip\"\n",
        "! unzip -q CRC-VAL-HE-7K.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECfWV0ToVZW2"
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "def format_test_data(example: dict[str, Any]) -> dict[str, Any]:\n",
        "    example[\"messages\"] = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": PROMPT,\n",
        "                },\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "    return example\n",
        "\n",
        "\n",
        "test_data = load_dataset(\"./CRC-VAL-HE-7K\", split=\"train\")\n",
        "test_data = test_data.shuffle(seed=42).select(range(1000))\n",
        "test_data = test_data.map(format_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f1Mfl6wWTHk"
      },
      "source": [
        "### Set up for evaluation\n",
        "\n",
        "Load the accuracy and F1 score metrics to evaluate the model's performance on the classfication task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdpSoOtLWkBX"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "# Ground-truth labels\n",
        "REFERENCES = test_data[\"label\"]\n",
        "\n",
        "\n",
        "def compute_metrics(predictions: list[int]) -> dict[str, float]:\n",
        "    metrics = {}\n",
        "    metrics.update(accuracy_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=REFERENCES,\n",
        "    ))\n",
        "    metrics.update(f1_metric.compute(\n",
        "        predictions=predictions,\n",
        "        references=REFERENCES,\n",
        "        average=\"weighted\",\n",
        "    ))\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdl6yTAzczn7"
      },
      "source": [
        "Define a postprocessing function to convert responses to integer class labels before computing metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5MEos4Bcy9L"
      },
      "outputs": [],
      "source": [
        "from datasets import ClassLabel\n",
        "\n",
        "# Rename the class names to the tissue classes, `X: tissue type`\n",
        "test_data = test_data.cast_column(\n",
        "    \"label\",\n",
        "    ClassLabel(names=TISSUE_CLASSES)\n",
        ")\n",
        "\n",
        "LABEL_FEATURE = test_data.features[\"label\"]\n",
        "# Mapping to alternative label format, `(X) tissue type`\n",
        "ALT_LABELS = dict([\n",
        "    (label, f\"({label.replace(': ', ') ')}\") for label in TISSUE_CLASSES\n",
        "])\n",
        "\n",
        "\n",
        "def postprocess(prediction: list[dict[str, str]], do_full_match: bool=False) -> int:\n",
        "    response_text = prediction[0][\"generated_text\"]\n",
        "    if do_full_match:\n",
        "        return LABEL_FEATURE.str2int(response_text)\n",
        "    for label in TISSUE_CLASSES:\n",
        "        # Search for `X: tissue type` or `(X) tissue type` in the response\n",
        "        if label in response_text or ALT_LABELS[label] in response_text:\n",
        "            return LABEL_FEATURE.str2int(label)\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh8t3-oUV9X7"
      },
      "source": [
        "### Compute baseline metrics on the pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj-8jpcPcPUm"
      },
      "source": [
        "Load the pretrained model using the `pipeline` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ged1Z8Tjb918"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pt_pipe = pipeline(\n",
        "    \"image-text-to-text\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Set `do_sample = False` for deterministic responses\n",
        "pt_pipe.model.generation_config.do_sample = False\n",
        "pt_pipe.model.generation_config.pad_token_id = processor.tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96XmD5loi6aX"
      },
      "source": [
        "Run batch inference on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cnCCpuSi6aX"
      },
      "outputs": [],
      "source": [
        "pt_outputs = pt_pipe(\n",
        "    text=test_data[\"messages\"],\n",
        "    images=test_data[\"image\"],\n",
        "    max_new_tokens=40,\n",
        "    batch_size=64,\n",
        "    return_full_text=False,\n",
        ")\n",
        "\n",
        "pt_predictions = [postprocess(out) for out in pt_outputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5slBRejjIOe"
      },
      "source": [
        "Compute metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6Rhs-0LfMPE"
      },
      "outputs": [],
      "source": [
        "pt_metrics = compute_metrics(pt_predictions)\n",
        "print(f\"Baseline metrics: {pt_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRuNOOAkWFCD"
      },
      "source": [
        "### Compute metrics on the fine-tuned model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwzxpFhsA0Yi"
      },
      "source": [
        "Load the base model with the fine-tuned LoRA adapter using the `pipeline` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsy5waU6jG7W"
      },
      "outputs": [],
      "source": [
        "ft_pipe = pipeline(\n",
        "    \"image-text-to-text\",\n",
        "    model=args.output_dir,\n",
        "    processor=processor,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Set `do_sample = False` for deterministic responses\n",
        "ft_pipe.model.generation_config.do_sample = False\n",
        "ft_pipe.model.generation_config.pad_token_id = processor.tokenizer.eos_token_id\n",
        "# Use left padding during inference\n",
        "processor.tokenizer.padding_side = \"left\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjw54wGjnWwT"
      },
      "source": [
        "Run batch inference on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjP2BFDcjJxN"
      },
      "outputs": [],
      "source": [
        "ft_outputs = ft_pipe(\n",
        "    text=test_data[\"messages\"],\n",
        "    images=test_data[\"image\"],\n",
        "    max_new_tokens=20,\n",
        "    batch_size=64,\n",
        "    return_full_text=False,\n",
        ")\n",
        "\n",
        "ft_predictions = [postprocess(out, do_full_match=True) for out in ft_outputs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_gxaSxRqWjG"
      },
      "source": [
        "Compute metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YmLRYMRfpon"
      },
      "outputs": [],
      "source": [
        "ft_metrics = compute_metrics(ft_predictions)\n",
        "print(f\"Fine-tuned metrics: {ft_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7Smu13_YlWG"
      },
      "source": [
        "# Next steps\n",
        "\n",
        "Explore the other [notebooks](https://github.com/google-health/medgemma/blob/main/notebooks) to learn what else you can do with the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fine_tune_with_hugging_face.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b743679e2dd41fc9c0f40f96e3f5638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6360f3f99cd744b68d807a6453138c8d",
              "IPY_MODEL_8e317d173460425282626390218a3ec2",
              "IPY_MODEL_a93179fe020f4ba79a1a0a82f9393d4c"
            ],
            "layout": "IPY_MODEL_924fbbfaf5604ce6881ccbdad739150d"
          }
        },
        "6360f3f99cd744b68d807a6453138c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3edc25e8609f48eca1f93022128ff1ad",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4b39329e821e418ba4e32c7e07b44bc5",
            "value": "Map:‚Äá100%"
          }
        },
        "8e317d173460425282626390218a3ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f4efb1878e84ebc9eab3801093804d8",
            "max": 22265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6017c8de1ef24c16a624f8ab55b1fe8b",
            "value": 22265
          }
        },
        "a93179fe020f4ba79a1a0a82f9393d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41f9af04ccc347faa649beba0e1b5729",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1c1f28cafc324a59b6a49daceda0bec8",
            "value": "‚Äá22265/22265‚Äá[00:01&lt;00:00,‚Äá11860.06‚Äáexamples/s]"
          }
        },
        "924fbbfaf5604ce6881ccbdad739150d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3edc25e8609f48eca1f93022128ff1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b39329e821e418ba4e32c7e07b44bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f4efb1878e84ebc9eab3801093804d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6017c8de1ef24c16a624f8ab55b1fe8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41f9af04ccc347faa649beba0e1b5729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c1f28cafc324a59b6a49daceda0bec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bd590bf677049c988e13d9b54278540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aca81fcf598f4cc2b3c9f5791fd1d962",
              "IPY_MODEL_a6ebf9aae3fc499981d34be75f32b6b9",
              "IPY_MODEL_0990c563b146486b9ec061f27c0c1493"
            ],
            "layout": "IPY_MODEL_d93213d572a047c6ad02b1d092dae6e9"
          }
        },
        "aca81fcf598f4cc2b3c9f5791fd1d962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f746f4a967f34048a818e8ee3306f7e4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_efabe38218e14596a7569900a8a576a7",
            "value": "Map:‚Äá100%"
          }
        },
        "a6ebf9aae3fc499981d34be75f32b6b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cce693c63cef4345b8ffe54bdacd2f79",
            "max": 3066,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20cbfd051ab7465a875a03bb076ac093",
            "value": 3066
          }
        },
        "0990c563b146486b9ec061f27c0c1493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81f988c2d5ba48cc82e9655ecb0f1ac9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8204f1b072154ebc9657289955faad76",
            "value": "‚Äá3066/3066‚Äá[00:00&lt;00:00,‚Äá11703.04‚Äáexamples/s]"
          }
        },
        "d93213d572a047c6ad02b1d092dae6e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f746f4a967f34048a818e8ee3306f7e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efabe38218e14596a7569900a8a576a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cce693c63cef4345b8ffe54bdacd2f79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20cbfd051ab7465a875a03bb076ac093": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81f988c2d5ba48cc82e9655ecb0f1ac9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8204f1b072154ebc9657289955faad76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93ab2e932ebd4ee0bb989ad75b0090bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44f46aa80cd447a0a444c2058401584d",
              "IPY_MODEL_8941fc5500644e9b9ba4c8629de5a837",
              "IPY_MODEL_2bf3935cc42d42a8815ae58977c87ab9"
            ],
            "layout": "IPY_MODEL_a9efd910d2ea4a63a7fe87cb33f6ba50"
          }
        },
        "44f46aa80cd447a0a444c2058401584d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a3e06d9fd5d4d8d8db57acf6038c7e4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6c02be820eeb4bd4b5604da2c3f60170",
            "value": "Loading‚Äáweights:‚Äá100%"
          }
        },
        "8941fc5500644e9b9ba4c8629de5a837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12ba7eefeb014c9d9e0d0729772c8747",
            "max": 883,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b851ca942874a5c82b72ef2f78302ec",
            "value": 883
          }
        },
        "2bf3935cc42d42a8815ae58977c87ab9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d683d0c67e594dfb96ed473900ef1f54",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5e3d4293df7a4a53b81a93684a79eb1a",
            "value": "‚Äá883/883‚Äá[00:36&lt;00:00,‚Äá90.84it/s,‚ÄáMaterializing‚Äáparam=model.vision_tower.vision_model.post_layernorm.weight]"
          }
        },
        "a9efd910d2ea4a63a7fe87cb33f6ba50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a3e06d9fd5d4d8d8db57acf6038c7e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c02be820eeb4bd4b5604da2c3f60170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12ba7eefeb014c9d9e0d0729772c8747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b851ca942874a5c82b72ef2f78302ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d683d0c67e594dfb96ed473900ef1f54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e3d4293df7a4a53b81a93684a79eb1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}